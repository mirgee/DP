%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,czech,american]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=4cm,bmargin=3cm,lmargin=3cm,rmargin=2cm,headheight=0.8cm,headsep=1cm,footskip=0.5cm}
\pagestyle{headings}
\setcounter{secnumdepth}{3}
\usepackage{url}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{svg}
\usepackage{outlines}
\usepackage[normalem]{ulem}
\usepackage{dirtytalk}
% \usepackage{subcaption}
% \usepackage{subfigure}

\makeatletter
\newenvironment{lyxlist}[1]
{\begin{list}{}
{\settowidth{\labelwidth}{#1}
 \setlength{\leftmargin}{\labelwidth}
 \addtolength{\leftmargin}{\labelsep}
 \renewcommand{\makelabel}[1]{##1\hfil}}}
{\end{list}}

\usepackage[varg]{txfonts}

\usepackage{indentfirst}

\clubpenalty=9500

\widowpenalty=9500

\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\code}{\texttt}

\newtheorem{thm}{Theorem} % [section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{conj}{Conjecture}
\newtheorem{dfn}{Definition}

\DeclareMathOperator{\id}{id}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% \DeclarePairedDelimiter\ceil{\lceil}{\rceil}
% \DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\def\code#1{\texttt{#1}}

\newcommand{\bd}[1]{\mathbf{#1}}
\newcommand{\RR}{\mathbb{R}}      
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\ZZP}{\mathbb{Z}+^}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}
\newcommand{\Tau}{\mathrm{T}}


\makeatother

\usepackage{babel}
\begin{document}

\def\documentdate{\today}

\pagestyle{empty}


\noindent \begin{center}
\begin{minipage}[c]{3cm}%
\noindent \begin{center}
\includegraphics[width=3cm,height=3cm,keepaspectratio]{Images/TITLE/cvut}
\par\end{center}%
\end{minipage}%
\begin{minipage}[c]{0.6\linewidth}%
\begin{center}
\textsc{\large{}Czech Technical University in Prague}{\large{}}\\
{\large{}Faculty of Nuclear Sciences and Physical Engineering}
\par\end{center}%
\end{minipage}%
\begin{minipage}[c]{3cm}%
\noindent \begin{center}
\includegraphics[width=3cm,height=3cm,keepaspectratio]{Images/TITLE/fjfi}
\par\end{center}%
\end{minipage}
\par\end{center}

\vspace{3cm}


\begin{center}
\textbf{\huge{}English Title}
\par\end{center}{\huge \par}

\vspace{1cm}


\selectlanguage{czech}%
\begin{center}
\textbf{\huge{}Czech Title}
\par\end{center}{\huge \par}

\selectlanguage{american}%
\vspace{2cm}


\begin{center}
{\large{}Diploma thesis}
\par\end{center}{\large \par}

\vfill{}

\begin{lyxlist}{MMMMMMMMM}
\begin{singlespace}
\item [{Author:}] \textbf{Miroslav Kovář}
\item [{Supervisor:}] \textbf{M.Sc. M.A. Sebastián Basterrech, Ph.D.}
\end{singlespace}

% \item [{Language~advisor:}] \textbf{Mgr. Jméno Učitelky Angličtiny}
\begin{singlespace}
\item [{Academic~year:}]2018/2019\end{singlespace}

\end{lyxlist}
\newpage{}

~\newpage{}

~

\vfill{}


\begin{center}
- Zadání práce -
\par\end{center}

\vfill{}


~\newpage{}

~

\vfill{}


\begin{center}
- Zadání práce (zadní strana) -
\par\end{center}

\vfill{}


~\newpage{}

\noindent \emph{\Large{}Acknowledgment:}{\Large \par}

\noindent Some acknoledgment here.

\vfill

\noindent \emph{\Large{}Author's declaration:}{\Large \par}

\noindent I declare that this research project is entirely
my own work and I have listed all the used sources in the bibliography.

\bigskip{}


\noindent Prague, \documentdate\hfill{}Miroslav Kovář

\vspace{2cm}


\newpage{}

~\newpage{}

\selectlanguage{czech}%
\begin{onehalfspace}
\noindent \emph{Název práce:}

\noindent \textbf{Czech Title}
\end{onehalfspace}

\bigskip{}


\noindent \emph{Autor:} Miroslav Kovář

\bigskip{}


\noindent \emph{Obor:} Aplikace přírodních věd \bigskip{}


\noindent \emph{Zaměření:} Matematická informatika

\bigskip{}


\noindent \emph{Druh práce:} Diplomová práce

\bigskip{}


\noindent \emph{Vedoucí práce:} M.Sc. M.A. Sebastián Basterrech, Ph.D.,
Artificial Intelligence Center, FEE, CTU Prague

\bigskip{}


% \noindent \emph{Konzultant:} doc. RNDr. Jméno Konzultanta, CSc., pracoviště
% konzultanta. Pouze pokud konzultant byl jmenován.

\bigskip{}


\noindent \emph{Abstrakt:} \bigskip{}


\noindent \emph{Klíčová slova:} 

\selectlanguage{american}%
\vfill{}
~

\begin{onehalfspace}
\noindent \emph{Title:}

\noindent \textbf{English Title}
\end{onehalfspace}

\bigskip{}


\noindent \emph{Author:} Miroslav Kovář

\bigskip{}


\noindent \emph{Abstract:} 

\bigskip{}


\noindent \emph{Key words:} 

%keywords in alphabetical order separated by commas

\newpage{}

~\newpage{}

\pagestyle{plain}

\tableofcontents{}

\newpage{}


\chapter*{Introduction}

% \addcontentsline{toc}{chapter}{Introduction}


\chapter{Classical EEG signal analysis methods}

\section{EEG signal}
Nature, processes in the brain, way of measuring, limitations, complications

Electroencephalography (EEG) is a noninvasive method of measuring fluctuations of electric potentials near the skull caused by synchronized firing of neurons in the upper cortical layers. Electroencephalogram is a record of these fluctuations measured over a period of time. \cite{nunez2006}

Although EEG has significantly lower spatial resolution in comparison with other diagnostic techniques such as functional magnetic resonance sampling (fMRI) and magnetoencephalography (MEG) \cite{srinivasan1999} and enables measuring only neural activity near the cortical surface, as a depression diagnostic tool, it has numerous benefits. Importantly, its significantly lower costs \cite{vespa1999} \cite{hamalainen1993}, high portability, and ease of operation imply increased availability to the patients \cite{schultz2012}. Moreover, it is perfectly noninvasive, which means less complications such as claustrophopia or anxiety \cite{murphy1997}.

Although the science of EEG signal analysis as a diagnostic tool brings compelling clinical promise as a result of the aforementioned benefits, it also presents multiple technical and conceptual challenges. 

\begin{figure} 
\centering
\noindent\makebox[\textwidth]{%
  \includegraphics[width=0.8\textwidth]{Images/stationarycomparison.png} }
  \caption{A comparison of stationary and non-stationary time series. (Courtesy: Protonk)}
\label{fig:statnonstatcomp}

\end{figure}
\begin{dfn}[\cite{priestley1988}]
  A series $\{X_t\}_{t \in \ZZ}$ is called \textbf{stationary}, if $\{X_t\}_{t \in \ZZ}$ for any set of times $t_1, t_2, \dots, t_n$ and any $k \in \mathrm{N}$, $P[X_{t_1}, X_{t_2}, \dots, X_{t_n}] = P[X_{t_1+k}, X_{t_2+k}, \dots, X_{t_n+k}]$, i.e. the joint probability distribution of $\{X_t\}_{t \in \ZZ}$ is not a function of time. It is called \textbf{non-stationary}, if it is not stationary.
\end{dfn}

\begin{dfn}[\cite{bickel1996}]
   A series $\{X_t\}_{t \in \ZZ}$ is called (noisy chaotic) \textbf{non-linear}, if it satisfies the relation
   \begin{align}
     X_t = f(X_{t-1}) + \epsilon_t
   \end{align}
   for a general $f : \RR \rightarrow \RR$.
\end{dfn}

%TODO: Read Kaplan and expand this section
EEG signals are prone to be infected with \emph{noise} due to imperfect isolation from surrounding environment. They are known to be \emph{transient, non-Gaussian, non-stationary and nonlinear} \cite{kaplan2005} \cite{stam2005}. Since some patterns do not activate relative to a stimulus, a successful classifier must be able to detect a pattern \emph{regardless of its starting time}, or find one. And finally, EEG records are relatively high dimensional - 16 electrodes sampling at 256 Hz result 4096 data points par second.

Moreover, due to the phenomenon of neural oscillations, patterns may appear in multiple frequency bands, from slow cortical potentials of $\delta$-waves at 0.5-4 Hz, to high $\gamma$ frequency band at 70-150 Hz. 

Patterns of oscillatory activity in various frequency band have been linked to various mental states \cite{canolty2006} \cite{buzsaki2004} and diseases such as epilepsy \cite{shusterman2008}, tremor \cite{mcauley2000}, Parkinson's disease and depression \cite{llinas1999}. Many of the diseases, including depression, share common oscillatory patterns known as thalamocortical dysrythmia, characterized by descrease in normal resting-state $\alpha$ (8-12 Hz) activity slowing down to $\theta$ (4-8 Hz) frequencies, accompanied by increase in $\beta$ and $\gamma$ (25-50 Hz) activity. \cite{vanneste2018}

\section{Non-linear time series analysis}
Modern neuroscientific work suggests that the most plausible research target for explaining the brain dynamics are the assemblies of coupled and synchronously active neurons, and since majority of those assemblies are describable by non-linear differential equations, it is common to apply principles derived from nonlinear dynamics to characterize these neuronal systems.\cite{kaplan2005} In this section, we will describe some of those principles.

Attractors, Poincare plots, recurrence plots, Lyapunov coefficients, fractal dimension, Hurst exponent, etc.

\begin{dfn}[\cite{kantz2004}]
  Let $\mathbf{x} \in \RR^m$ be an $m \in NN$ dimensional state space vector dependent on time. A deterministic \textbf{dynamical system} is described by a set of $m$ first-order differential equations
  \begin{align}
    \frac{d}{dt}\mathbf{x}(t) = \mathbf{f}(\mathbf{x}(t)), \qquad t \in \RR
  \end{align}
\end{dfn}

In other words, a dynamical system is a model that determines the evolution of a system only given by the initial state, and current state is a function of the previous state. Hence, a total description of a dynamical system is given by the initial state and a set of equations. A non-linear dynamical system is a system where the differential equations describing its dynamics are non-linear. Unlike in a linear system, changes in the initial state of a non-dynamical system are allowed to have a non-linear relationship to the state space trajectory of the system. \cite{kaplan2005}

\subsection{Attractor}
\begin{dfn}[\cite{kantz2004}]
  A dynamical system is called dissipative, when it is the case that
  \begin{align}
    E[\mathrm{div} \mathbf{f}] < 1
  \end{align}
\end{dfn}

In other words, average space space volume (containing the initial state) is contracted as the system evolves.

Dissipative dynamical system have the property that a set of initial states (of positive measure) will converge to a subspace of the overall state space. This subspace is a geometrical object called an \textbf{attractor}. Example of four basic attractors can be seen on Figure \ref{fig:attractors}.

\begin{figure} 
\centering
\noindent\makebox[\textwidth]{%
  \includegraphics[width=0.8\textwidth]{Images/attractors.png} }
  \caption{Visualization of four common attractor types (units are arbitrary). Left to right, top to bottom:
    \emph{Point attractor} is the only type of attractor of linear deterministic dissipative systems.
    \emph{Limit cycle} corresponds to a peridic system.
    \emph{Torus attractor} corresponds to a quasi-periodic system, resulting (in this example) from a superposition of two periodic oscillations.
    \emph{Chaotic} (strange) \emph{attractor}, characterized by strong dependence on the original conditions. (\cite{stam2005})}
\label{fig:attractors}
\end{figure}

Since most physiogenerated signals are chaotic, their analysis is concerned primarily with \emph{chaotic} (strange) \emph{attractors}. These attractors are complex, and exhibit what is known as \emph{fractal geometry}. For our purposes, this means they can be characterized as having quantifiable self-similarity.\footnote{Cantor set being a canonical example of self-similarity.} An example of a self-similar attractor is shown on Figure \ref{fig:self-similarity}.

\begin{figure} 
\centering
\noindent\makebox[\textwidth]{%
  \includegraphics[width=0.8\textwidth]{Images/self-similarity.png}}
  \caption{Noise-reduced visualization of successive enlargements of highly self-similar attractor. (\cite{kantz2004})}
\label{fig:self-similarity}
\end{figure}

One possible approach to non-linear time series analysis consists of the following steps: \cite{stam2005}
\begin{enumerate}
  \item reconstruction of the dynamics of given system from recorded data,
  \item characterization of the reconstructed attractor,
  \item checking validity of the results with surrogate data testing.
\end{enumerate}

In the following text, we will describe multiple techniques of characterizing the properties of attractors (and hence ways of describing the dynamics of the dynamical system).

\subsection{Recurrence plot}
Recurrence plot is a method of characterizing the attractor underlying a chaotic process.

\section{Frequency domain}
Wavelets, sliding window

\section{Deep learning}
Results in applying deep learning to EEG signal analysis

\chapter{CNNs and CapsNets}

\section{CNNs}
\subsection{History}

The classical approach to image pattern recognition consists of the following stages:
\begin{description}
  \item[preprocessing:] supressing unwanted distortions and noise, enhancement beneficient for further processing,
  \item[object segmetation:] separating disparate objects from the background,
  \item[feature extraction:] gathering relevant information about the properties of the objects, removing irrelevant variations,
  \item[classification:] categorizing segmented objects based on obtained features into classes.
\end{description}

The preprocessing step may require additional assumptions about the data or further processing, which are potentially too restrictive or too broad. Getting around this limitation requires dealing with complications such as high dimensionality of the input (number of pixels) and desirability of invariance towards a number of allowable distortions and geometrical transformations.

Artificial neural networks in combination with gradient-based learning are one possible solution to the problem. By gradually optimizing a set of weights based on a training data set using a differentiable error function, they provide a framework for learning a suitable set of assumptions automatically from the data.

One of the oldest neural network architectures, fully connected multi-layer perceptron (FC-MLP), can be used for image pattern recognition. However, it has the following drawbacks:
\begin{description}
  \item[parameter explosion:] the number of parameters of such network is exponential in the number of layers, increasing the capacity of the network and therefore need for more data,
  \item[no invariance:] no invariance even with respect to common geometrical transformation such as translation, rotation and scaling,
  \item[ignoring input topology:] natural images exhibit strong local structure and high correlation between intensities of neighboring pixels, but FC-MLPs are unstructed - inputs can be presented in any order.
\end{description}

Although the main idea dates back 1980 with K. Fukushima's neocognitron \cite{fukushima1982}, the back-propagation algorithm was not known at the time. The first convolutional architecture successfuly applied on an image pattern recognition problem by attempting to solve the aforementioned problems, dubbed LeNet-5, was proposed in 1998 by Y. LeCun, L. Bottou, Y. Bengio and P. Haffner \cite{LeCun1998}.

\subsection{Description}

\begin{figure} 
\centering
\noindent\makebox[\textwidth]{%
  \includegraphics[width=0.8\textwidth]{Images/lenet-5.png} }
  \caption{LeNet-5 architecture \cite{lecun1999}.}
\label{fig:lenet-5}
\end{figure}

Bearing resemblence to visual processing in biological organisms \footnote{As early as in 1968, D. H. Hubel and T.N. Wiesel discovered that some cells (called simple cells) in cat's primary visual cortex (V1) with small receptive fields (shared by neighboring neurons) are sensitive to straight lines and edges of light of particular orientation, and other cells (called complex cells) with larger receptive fields further in the visual cortex also respond to straight lines and edges, but with invariance to translation \cite{Hubel1968}.}, LeNet-5 proposed the following design principles to enforce \emph{shift, scale and distortion invariance}: \cite{lecun1999}
\begin{description}
  \item[local receptive fields:] each neuron in a layer receives input from a small neighborhood in the previous layer,
  \item[shared weights:] each layer is composed of neurons organized in planes within which each neuron have the same weight vector (feature map),
  \item[spatial subsampling:] adding a subsampling layers, which reduce the resolution of the previous layer by averaging or taking the maximal value of neighboring pixels in the previous layer.
\end{description}

\emph{Local receptive fields} enable the network to synthesize filters that produce strong response to elementary salient features in the early layers (such as lines, edges and corners in a visual input, and equivalence in other modalities), and then learn to combine them in the subsequent layers to produce higher-order feature detectors. 

\emph{Shared weights} principle exploits the fact that translation-invariant features have the property that one feature detector can be used accross the entire image. Since neural units in a layer with differing receptive fields possess the same feature map, the same feature detecting operation (convolution with feature map kernel followed by additive bias and a application of a non-linear function) is performed on differing parts of the image. A single convolutional layer is composed of multiple feature detecting planes.

\emph{Spatial subsumpling} is purposed to ensure scale and distortion invariance\footnote{Whether it achieves this goal has been famously doubted by Geoffrey Hinton: ``The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.'' \cite{}} by reducing the precision at which a feature is encoded in a feature map by reducing its resolution - when scale and distortion invariance is assumed, the exact location of a feature becomes less important and is allowed to exhibit slight positional variance. Although this simple solution performs well in many practical situations, it has multiple drawbacks, as mentioned in section \ref{sec:capsnets} (relative representation of features is not encoded in the feature map representation)


\subsection{Applications}
Maybe mention an example of how LeNet-5 was improved on subsequently (AlexNet, ResNet, etc.)


\section{CapsNets} \label{sec:capsnets}
Does it make sense trying them? I found a only a few successful implementations. Maybe it would be better to try those after we have some results already, because it seems risky - we might end up with nothing.


\chapter{Experiments}

\section{Dataset}
Size of our dataset, conditions during trials, labels, etc.

\section{Results}


\chapter*{Conclusion}


\bibliographystyle{plain}
\bibliography{refs}
\end{document}
